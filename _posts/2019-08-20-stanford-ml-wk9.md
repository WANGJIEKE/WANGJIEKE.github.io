---
layout: post
title:  "斯坦福机器学习课程笔记（Week 9）"
date:   2019-08-20 23:23:00 -0700
tags:   study-cs machien-learning
---

## 本系列的其它文章

- [斯坦福机器学习课程笔记（Week 1）]({% post_url 2019-06-25-stanford-ml-wk1 %})
- [斯坦福机器学习课程笔记（Week 2）]({% post_url 2019-07-05-stanford-ml-wk2 %})
- [斯坦福机器学习课程笔记（Week 3）]({% post_url 2019-07-13-stanford-ml-wk3 %})
- [斯坦福机器学习课程笔记（Week 4）]({% post_url 2019-07-15-stanford-ml-wk4 %})
- [斯坦福机器学习课程笔记（Week 5）]({% post_url 2019-07-25-stanford-ml-wk5 %})
- [斯坦福机器学习课程笔记（Week 6）]({% post_url 2019-08-01-stanford-ml-wk6 %})
- [斯坦福机器学习课程笔记（Week 7）]({% post_url 2019-08-05-stanford-ml-wk7 %})
- [斯坦福机器学习课程笔记（Week 8）]({% post_url 2019-08-12-stanford-ml-wk8 %})
- **斯坦福机器学习课程笔记（Week 9）**
- [斯坦福机器学习课程笔记（Week 10）]({% post_url 2019-08-29-stanford-ml-wk10 %})
- [斯坦福机器学习课程笔记（Week 11）]({% post_url 2019-08-29-stanford-ml-wk11 %})

## 反常检测（Anomaly Detection）

所谓反常检测，简单来讲就是，根据已知的正常数据集$$\{x^{(1)},x^{(2)},\dots,x^{(m)}\}$$来判断新的数据$$x_\text{test}$$是否反常。一般的做法是，训练出模型$$P(x)$$，然后检测是否$$P(x)<\epsilon$$（这里的$$\epsilon$$可以看成是某个阈值）。

反常检测可以被运用在信用卡欺诈检测，残次品检测，数据中心的服务器设备监控等。

### 正态分布（Normal Distribution）

若随机变量$$X\in\mathbb{R}$$服从正态分布（也称为高斯分布，Gaussian Distribution），且平均值为$$\mu$$，方差为$$\sigma^2$$（可以表示为$$X\sim\mathcal{N}(\mu,\sigma^2)$$，其中的“$$\sim$$”符号代表“服从……分布”），那么$$X$$的概率密度函数（Probability Density Function）为

$$
P(X=x;\mu,\sigma^2)=\frac1{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
$$

图像如下（其中红色线代表标准正态分布，即$$\mu=0,\sigma^2=1$$的正态分布，图片来自[维基百科](https://zh.wikipedia.org/wiki/File:Normal_Distribution_PDF.svg)）

![Normal Distribution PDF](/assets/2019-08-20-stanford-ml-wk9/Normal_Distribution_PDF.svg)

如果我们只有数据点，想要知道对应的正态分布，我们可以通过[最大似然估计](https://zh.wikipedia.org/wiki/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)来得到参数的估计值。由最大似然估计推导出的公式如下

$$
\begin{eqnarray*}
\hat\mu&=&\frac1m\sum_{i=1}^m \\
\hat{\sigma}^2&=&\frac1m\sum_{i=1}^m(x^{(i)}-\hat\mu)^2
\end{eqnarray*}
$$

### 算法

假设我们的训练集有$$m$$个样本，每个样本$$x\in\mathbb{R}^n$$的“反常概率”用$$P(x)$$表示。设$$x$$第$$j$$个*独立*特征$$x_j\sim\mathcal{N}(\mu_j,\sigma_j^2)$$，那么$$P(x)$$可以表示为

$$
\begin{eqnarray*}
P(x)&=&P(x_1;\mu_1,\sigma_1^2)P(x_2;\mu_2,\sigma_2^2)\cdots P(x_n;\mu_n,\sigma_n^2)\\
&=&\prod_{j=1}^nP(x_j;\mu_j,\sigma_j^2)
\end{eqnarray*}
$$

求这个$$P(x)$$的过程也被称为密度预测（Density Estimation）。

完整的反常检测算法如下

1. 选择适当的特征（这个特征能很好地区分是否反常）
2. 计算出相应的$$\mu_1,\dots,\mu_n,\sigma_1^2,\dots,\sigma_n^2$$。
3. 对于新的数据，计算出发生数据所示情况的概率，若概率小于某个阈值，则认为这个数据反常。

### 例子

这里举一个双变量的协同正态分布的例子。

![image-20190819205352173](/assets/2019-08-20-stanford-ml-wk9/image-20190819205352173.png)

因为这是两个随机变量的协同正态分布（两个*独立*的特征），所以$$P(x_1, x_2)$$图像是左下角的一个三维的曲面。

假设我们的$$\epsilon=0.02$$，此外我们有两个新的要验证是否反常的数据点$$x_\text{test}^{(1)}$$和$$x_\text{test}^{(2)}$$。我们只需要计算$$P(x_\text{test}^{(1)})$$和$$P(x_\text{test}^{(2)})$$，如果小于$$\epsilon$$，则认为是反常的；反之则认为是不反常的。

![image-20190819210136416](/assets/2019-08-20-stanford-ml-wk9/image-20190819210136416.png)

如果我们将边界表示出来，那么紫色阴影部分就是反常范围。

### 反常检测系统设计

跟之前提到的一样，我们在这里也需要

- 一个能评价系统表现好坏的**数字**
- 有标识（是否反常）的数据集
- 训练集、交叉验证集、测试集

其中训练集，交叉验证集和测试集的划分跟以前比有小小改动。正常样本的划分还是按照测试集60%，交叉验证集和测试集各20%的方式来划分。但是对于反常样本（通常只占很小一部分），我们不将它们放入训练集，而是对半分放入交叉验证集和测试集。

![image-20190819215223044](/assets/2019-08-20-stanford-ml-wk9/image-20190819215223044.png)

讲完了如何划分数据，接下来是如何评估我们的算法。吴教授建议我们可以采用$$F_1$$分数的方法来进行评估。此外，我们也可以通过交叉验证集来调整我们阈值$$\epsilon$$的大小。

总之，因为这是非常偏斜的数据（正常样本占绝大多数），我们可以利用在第六周所学的处理偏斜数据的知识来帮助我们改进算法。

### 反常检测与监督学习（分类器）

| 反常检测                                        | 监督学习（分类器）                |
| ----------------------------------------------- | --------------------------------- |
| 数据偏斜（异常，即$$y=1$$，的数量少）           | 数据不偏斜                        |
| 导致异常的原因各不相同，难以总结出为什么$$y=1$$ | 数据足够让算法找出使$$y=1$$的特征 |

### 特征的处理和选择

有时候我们的特征本身并不满足正态分布，这时候最简单的做法是取对数或者开根号。

![image-20190819221942622](/assets/2019-08-20-stanford-ml-wk9/image-20190819221942622.png)

此外，有时候我们会碰到我们的概率模型对一个反常样本给出了一个较高的概率。这时候我们应该单独检查这个反常样本，寻找其它能够将它与正常样本区分开来的特征。

![image-20190819222331973](/assets/2019-08-20-stanford-ml-wk9/image-20190819222331973.png)

例如在上图的例子中，绿色的数据点是反常样本。但是我们发现当只有一个特征$$x_1$$的时候，这个反常样本跟正常样本混在一起了。于是我们检查分类出错的原因，并得到了新的特征$$x_2$$。当我们使用$$x_1$$和$$x_2$$的协同正态分布作为我们的概率模型时，我们就将绿色的反常样本区分开了。

此外，我们也可以将特征进行线性组合，得到新的特征。

![image-20190819222819987](/assets/2019-08-20-stanford-ml-wk9/image-20190819222819987.png)

例如上图中的$$x_5$$，通过让CPU负荷除以网络流量，我们能够得到用来识别“CPU占用大但是网络流量低”这种情况的特征。

### 多元正态分布（Multivariate Normal Distribution）

![image-20190819224524962](/assets/2019-08-20-stanford-ml-wk9/image-20190819224524962.png)

只使用多随机变量的协同正态分布并不能很好地检测出反常样本。在上图的例子中，这个正态分布的图像截面是紫色的圆，而不是蓝色的椭圆。这就导致了本来不应该算作正常样本的绿色数据点被误判了。

为了解决这个问题，我们可以使用**[多元正态分布](https://zh.wikipedia.org/wiki/%E5%A4%9A%E5%85%83%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83)**。在之前讲到的正态分布中，我们将每个特征看成是一个独立的随机变量，得到的正态分布是多个独立随机变量的协同正态分布。而多元正态分布中，我们直接将$$x\in\mathbb{R}^n$$的所有的特征看成一个整体（一个随机向量）。

$$
P(x;\mu,\Sigma)=\frac1{\sqrt{(2\pi)^n\vert\Sigma\vert}}e^{-\frac12(x-\mu)^T\Sigma^{-1}(x-\mu)}
$$

其中$$\mu\in\mathbb{R}^n$$和$$\Sigma\in\mathbb{R}^{n\times n}$$都是参数。这里的$$\Sigma$$是协方差矩阵。$$\vert\Sigma\vert$$代表$$\Sigma$$的行列式（Determinant）。下面是协方差矩阵的定义（可以参考上周笔记中关于协方差矩阵的部分）

$$
\Sigma=\begin{bmatrix}
\mathrm{cov}(x_1,x_1)& \cdots& \mathrm{cov}(x_1,x_n) \\
\vdots& \ddots& \vdots \\
\mathrm{cov}(x_n,x_1)& \cdots& \mathrm{cov}(x_n,x_n)
\end{bmatrix}
$$

实际上，多个独立随机变量的协同正态分布，是多元正态分布的一种特殊形式。之所以强调*独立*随机变量，是因为，如果这些随机变量是独立的（独立必定不相关，则协方差为0），则协方差矩阵是对角矩阵。

下面是一些拥有不同参数的多元正态分布图像

![image-20190819230759275](/assets/2019-08-20-stanford-ml-wk9/image-20190819230759275.png)

![image-20190819230829795](/assets/2019-08-20-stanford-ml-wk9/image-20190819230829795.png)

注意到以上例子中的协方差矩阵都是对角矩阵。这代表$$x_1$$和$$x_2$$是不相关的。当然，$$x_1$$和$$x_2$$也可以是相关的，在这种情况下，多元正态分布图像如下（右侧的两个）

![image-20190819231039060](/assets/2019-08-20-stanford-ml-wk9/image-20190819231039060.png)

$$x_1$$和$$x_2$$之间当然也可以是负相关的。

![image-20190819231147493](/assets/2019-08-20-stanford-ml-wk9/image-20190819231147493.png)

均值$$\mu$$的不同也会让图像的中心点发生变化。

![image-20190819231334672](/assets/2019-08-20-stanford-ml-wk9/image-20190819231334672.png)

### 在反常检测中运用多元正态分布

类似一元正态分布，我们可以用类似的方法得到均值的预测值和协方差矩阵的预测值。

$$
\hat\mu=\frac1m\sum_{i=1}^mx^{(i)}\qquad\hat\Sigma=\frac1m\sum_{i=1}^m(x^{(i)}-\hat\mu)(x^{(i)}-\hat\mu)^T
$$

得到参数的预测值之后，我们就可以通过同样的方法（判断$$P(x)<\epsilon$$是否为真）来检测$$x$$是否反常了。

至于我们要不要将随机变量（特征）之间的相关性纳入考虑（即是否使用多元正态分布），还是说强行认为两个无关（而使用多随机变量的协同正态分布），则需要我们了解两种实践的区别与联系。

| 原始模型（多独立随机变量的协同正态分布）                     | 多元正态分布                                              |
| ------------------------------------------------------------ | --------------------------------------------------------- |
| 如果特征之间存在相关性，我们需要自己手动弄出新特征以进行判断（比如CPU和网络流量的那个例子） | 能够自动处理特征之间的相关性                              |
| 需要的算力低                                                 | 需要的算力高                                              |
| 不需要太多的数据（当$$m$$较小时依然可以使用）                | 需要保证$$m\ge10n$$，否则协方差矩阵$$\Sigma$$可能会不可逆 |

## 推荐系统

### 预测用户评分

假设用户评分范围是0到5分，我们有五部电影，三部爱情喜剧片，两部动作片，同时我们有四位用户，有些用户可能没有看过某些电影（没有看过的电影的分数用问号“?”表示）。

![image-20190820171816391](/assets/2019-08-20-stanford-ml-wk9/image-20190820171816391.png)

根据用户的过往表现，我们可以推测，Alice和Bob很有可能会给没看过的爱情喜剧片打高分，而Carol会给爱情喜剧片打低分，Dave会给没看过的爱情喜剧片打低分，给动作片打高分。

### 基于内容的推荐系统

在上面的例子中，我们可以根据我们的经验来预测用户的打分。同样，我们可以设计一套算法，让计算机来进行预测。

假设我们的电影有两个特征，$$x_1$$代表了这部电影的“爱情程度”，$$x_2$$代表了这部电影的“动作程度”，我们可以将每一部电影看成是一个向量。此外，对于每一个用户$$j$$，我们也要学习出参数$$\theta^{(j)}\in\mathbb{R}^{n+1}$$，这样我们就能通过$$(\theta^{(j)})^Tx^{(i)}$$得出用户$$j$$可能会给电影$$i$$打出的分数了。

学习参数$$\theta^{(j)}$$的方法如下

$$
\min_{\theta^{(j)}}\frac1{2m^{(j)}}\sum_{i:r(i,j)=1}\Big((\theta^{(j)})^T(x^{(i)})-y^{(i,j)}\Big)^2+\frac\lambda{2m^{(j)}}\sum_{k=1}^n(\theta_k^{(j)})^2
$$

其中$$m^{(j)}$$代表用户$$j$$已评分的电影数量。如果要同时学习所有用户的参数（即学习$$\theta^{(1)},\dots,\theta^{(n_u)}$$）

$$
\min_{\theta^{(1)},\dots,\theta^{n_u}}\frac12\sum_{j=1}^{n_u}\sum_{i:r(i,j)=1}\Big((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\Big)^2+\frac\lambda2\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta_k^{(j)})^2
$$

对应的梯度下降公式为

$$
\begin{eqnarray*}
\theta_0^{(j)}&:=&\theta_0^{(j)}-\alpha\sum_{i:r(i,j)=1}\Big((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\Big)x_0^{(i)} \\
\theta_k^{(j)}&:=&\theta_k^{(j)}-\alpha\bigg(\sum_{i:r(i,j)=1}((\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_k^{(i)}+\lambda\theta_k^{(j)}\bigg)\qquad\text{(}k=1,\dots,n\text{)}
\end{eqnarray*}
$$

### 协同过滤（Collaborative Filtering）

在前面的例子里，我们需要手工为每部电影设置特征$$x_1$$和$$x_2$$，非常麻烦。协同过滤是一种算法，能从已知的$$\theta^{(1)},\dots,\theta^{(n_u)}$$，得到电影的特征$$(x_1^{(1)},x_2^{(1)}),\dots,(x_1^{(n_m)},x_2^{(n_m)})$$。具体公式如下

$$
\min_{x^{(i)}}\frac12\sum_{j:r(i,j)=1}\Big((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\Big)^2+\frac\lambda2\sum_{k=1}^n(x_k^{(i)})^2
$$

类似地，如果我们想要同时学习所有电影的特征，公式如下

$$
\min_{x^{(1)},\dots,x^{(n_m)}}\frac12\sum_{i=1}^{n_m}\sum_{j:r(i,j)=1}\Big((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\Big)^2+\frac\lambda2\sum_{i=1}^{n_m}\sum_{k=1}^n(x_k^{(i)})^2
$$

我们发现，协同过滤和基于内容推荐是个类似鸡和蛋的问题。吴教授这里的建议是先从$$\theta$$开始优化，然后优化$$x$$，再优化$$\theta$$，再到$$x$$，如此循环。

### 协同过滤算法实现

我们可以通过下面的公式实现同时优化$$\theta$$和$$x$$

$$
J(x^{(1)},\dots,x^{(n_m)},\theta^{(1)},\dots,\theta^{(n_u)})=\frac12\sum_{(i,j):r(i,j)=1}\Big((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\Big)^2+\frac\lambda2\sum_{i=1}^{n_m}\sum_{k=1}^n(x_k^{(i)})^2+\frac\lambda2\sum_{j=1}^{n_u}\sum_{k=1}^n(\theta_k^{(j)})^2\\

\min_{x^{(1)},\dots,x^{(n_m)},\\
\theta^{(1)},\dots,\theta^{(n_u)}}
J(x^{(1)},\dots,x^{(n_m)},\theta^{(1)},\dots,\theta^{(n_u)})
$$

具体来说就是，首先，进行随机初始化。然后，使用梯度下降法进行优化

$$
x_k^{(i)}:=x_k^{(i)}-\alpha\bigg(\sum_{j:r(i,j)=1}\Big((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\Big)\theta_k^{(j)}+\lambda x_k^{(i)}\bigg)\\
\theta_k^{(j)}:=\theta_k^{(j)}-\alpha\bigg(\sum_{j:r(i,j)=1}\Big((\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\Big)x_k^{(i)}+\lambda \theta_k^{(j)}\bigg)
$$

（这里的$$x$$和$$\theta$$没有$$x_0$$和$$\theta_0$$）

最后，对于用户$$j$$，他/她对电影$$i$$的分数的预测值为$$(\theta^{(j)})^T(x^{(i)})$$。

### 协同过滤算法的向量化实现

我们可以把分数（包括未打分的）放在一个矩阵$$Y$$里。

![image-20190820183724060](/assets/2019-08-20-stanford-ml-wk9/image-20190820183724060.png)

我们按照如下方式将电影的特征和用户的喜好放在矩阵里

$$
X=\begin{bmatrix}
—(x^{(1)})^T— \\
\vdots \\
—(x^{(n_m)})^T—
\end{bmatrix}\qquad\Theta=\begin{bmatrix}
—(\theta^{(1)})^T— \\
\vdots \\
—(\theta^{(n_u)})^T—
\end{bmatrix}
$$

那么，我们可以得到预测用户评分的矩阵

$$
X\Theta^T=\begin{bmatrix}
(\theta^{(1)})^T(x^{(1)})& \cdots& (\theta^{(n_u)})^T(x^{(1)}) \\
\vdots& \ddots& \vdots \\
(\theta^{(1)})^T(x^{(n_m)})& \cdots& (\theta^{(n_u)})^T(x^{(n_m)})
\end{bmatrix}
$$

通过将$$Y$$分解为$$X\Theta^T=Y'$$，用低秩矩阵$$Y'$$来近似$$Y$$的做法，就叫做低秩矩阵分解（Low Rank Matrix Factorization）。

### 寻找相关的电影

我们能通过算法学习到每个电影的特征$$x$$，但是我们很难弄明白这些特征的含义是什么。不过我们可以通过计算两个电影在空间中的“距离”来判断两部电影是否类似。

比如说有电影$$i$$，我们想找出与它有关的电影$$j$$。那么我们可以计算$$\vert\vert x^{(i)}-x^{(j)}\vert\vert$$，然后取结果较小的作为相关电影。

### 均值标准化（Mean Normalization）

假设有新用户Eve，她还没对任何电影作出评价。回忆“代价函数”的计算公式，因为这位用户没有评分，所以代价函数中的误差项对这位用户的$$\theta$$没有影响。但是在后面的正规化项中，这位用户的$$\theta$$会被设为0，导致系统在预测该用户评分时，对于任意电影给出的评分都是0。

![image-20190820191545353](/assets/2019-08-20-stanford-ml-wk9/image-20190820191545353.png)

很明显这种做法特别憨，因为我们无法给她推荐电影。

为了解决这个问题，我们可以首先求出每部电影的平均评分$$\mu$$，然后再让每个人的评分减去这个平均评分（即计算$$Y-\mu$$），让电影的平均评分变成0分（问号在计算时算作0，但是不会被替换）。

我们从$$Y-\mu$$中学习到$$\Theta$$和$$X$$之后，可以反过来求新的$$Y$$。

$$
Y=X\Theta^T+\mu
$$

对于我们的新用户Eve来说，她对于电影$$i$$的预测评分是

$$
(\theta^{(5)})^T(x^{(i)})+\mu_i
$$

### 补充：低秩矩阵分解的简单介绍

参考资料：[低秩分解](https://www.cnblogs.com/missidiot/p/9869182.html)

矩阵的秩（rank）定义是矩阵中线性无关的行数或者列数。矩阵$$A$$的秩用$$\text{rk}(A)$$表示。如果我们把矩阵看成线性方程组，那么矩阵的秩就是“有用”的线性方程组的数量。假设我们有下面的线性方程组

$$
\begin{cases}
x_1+x_2=0 \\
3x_1-5x_2=8
\end{cases}
$$

利用矩阵乘法，我们可以将上面的方程组用矩阵表示

$$
A=\begin{bmatrix}
1& 1\\
3& -5\\
\end{bmatrix}\qquad x=\begin{bmatrix}x_1\\x_2\end{bmatrix}\qquad b=\begin{bmatrix}0\\8\end{bmatrix}\qquad(A\vert b)=\begin{bmatrix}1 &1 &0\\3 &-5 &8\end{bmatrix}\\
Ax=b
$$

这个问题小学生也能解出来，答案是$$x=\begin{bmatrix}1\\-1\end{bmatrix}$$。

但是如果我换一个方程组

$$
A=\begin{bmatrix}1 &-1\\-3 &3\end{bmatrix}\qquad b=\begin{bmatrix}7\\-21\end{bmatrix}\qquad (A\vert b)=\begin{bmatrix}1 &-1 &7\\-3 &3 &-21\end{bmatrix}
$$

我们会发现，满足$$Ax=b$$的$$x$$有无数个。如果我们观察上面两个方程组对应的增广矩阵（Augmented Matrix）$$(A\vert B)$$，我们会发现，在有无数个解的情况下，对应的$$(A\vert b)$$的行之间，是倍数关系，在这个例子里，第二行的每个数是第一行中对应的数乘以-3，也就是说，这两个方程，$$x_1-x_2=7$$和$$-3x_1+3x_2=-21$$，对应是同一条直线。换句话说，这两个方程里，只有一个方程是“有用”的，也就是说，$$(A\vert b)$$的秩是1。

设$$A$$是$$m\times n$$的矩阵。如果$$\text{rk}(A)=\text{rk}(A\vert b)$$，那么对应的方程组有解。若$$\text{rk}(A)=\text{rk}(A\vert b)=n$$，则对应的方程组只有一个解。若$$\text{rk}(A)=\text{rk}(A\vert b)<n$$，则对应的方程组有无数个解。

说了这么多，主要是要能体会到，一个矩阵的秩，表示了这个矩阵到底有多少“实打实”的信息。了解了什么是秩之后，低秩的概念也就比较容易懂了。低秩的意思就是，秩小于行数和列数，它告诉我们，这个矩阵里的“有用”的部分较少，换句话说就是，“冗余”的部分较多。那么对于一个只知道部分值的低秩矩阵，我们就可以通过它里面的“冗余”部分，来求出缺失部分的近似值。如果用视频里的例子，就是通过其他用户的喜好和电影的特征，来预测未评分用户可能会给出的评分。

那我们凭什么说用户评分矩阵$$Y$$是低秩的呢？简单来说就是，电影和电影之间，用户和用户之间，是有共性的，或者说，很大概率某个用户/电影可以被表示为其他用户/电影的线性组合。

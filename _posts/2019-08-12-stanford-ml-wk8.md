---
layout: post
title:  "斯坦福机器学习课程笔记（Week 8）"
date:   2019-08-12 00:54:00 -0700
tags:   study-cs machien-learning
---

## 本系列的其它文章

- [斯坦福机器学习课程笔记（Week 1）]({% post_url 2019-06-25-stanford-ml-wk1 %})
- [斯坦福机器学习课程笔记（Week 2）]({% post_url 2019-07-05-stanford-ml-wk2 %})
- [斯坦福机器学习课程笔记（Week 3）]({% post_url 2019-07-13-stanford-ml-wk3 %})
- [斯坦福机器学习课程笔记（Week 4）]({% post_url 2019-07-15-stanford-ml-wk4 %})
- [斯坦福机器学习课程笔记（Week 5）]({% post_url 2019-07-25-stanford-ml-wk5 %})
- [斯坦福机器学习课程笔记（Week 6）]({% post_url 2019-08-01-stanford-ml-wk6 %})
- [斯坦福机器学习课程笔记（Week 7）]({% post_url 2019-08-05-stanford-ml-wk7 %})
- **斯坦福机器学习课程笔记（Week 8）**
- [斯坦福机器学习课程笔记（Week 9）]({% post_url 2019-08-20-stanford-ml-wk9 %})
- *斯坦福机器学习课程笔记（Week 10）（敬请期待）*
- *斯坦福机器学习课程笔记（Week 11）（敬请期待）*

## 聚类算法（Clustering Algorithm）

### 非监督学习

对于非监督学习，我们的目标是找到数据的某种结构。聚类算法是其中的一种非监督学习算法。对于非监督学习来说，我们的数据集是类似$$\{x^{(1)},x^{(2)},\cdots,x^{(m)}\}$$的样子，不存在$$y^{(i)}$$。

### 了解K-平均算法（K-Means Algorithm）

#### 算法的直观理解

假设我们要将下面的数据点分成两类

![image-20190807173230572](/assets/2019-08-12-stanford-ml-wk8/image-20190807173230572.png)

我们可以随意挑选两个聚类中心（Cluster Centroid）。下图中红色和蓝色的交叉符号代表我们任意选择的两个聚类中心。

![image-20190807173333526](/assets/2019-08-12-stanford-ml-wk8/image-20190807173333526.png)

随后，我们将绿点涂色，更靠近蓝色交叉的涂为蓝色，靠近红色交叉的涂为红色。

![image-20190807173439580](/assets/2019-08-12-stanford-ml-wk8/image-20190807173439580.png)

我们分别将所有红点和蓝点的“均值”算出，并将红色和蓝色的交叉移到这个均值点。

![image-20190807173618981](/assets/2019-08-12-stanford-ml-wk8/image-20190807173618981.png)

移动后，我们根据与新的红蓝交叉的距离，重新给数据点涂色。

![image-20190807173740038](/assets/2019-08-12-stanford-ml-wk8/image-20190807173740038.png)

涂色后，重新计算并调整中心的位置。

![image-20190807173849497](/assets/2019-08-12-stanford-ml-wk8/image-20190807173849497.png)

持续这个过程，最终我们的中心会到达一个稳定的位置。

![image-20190807174016434](/assets/2019-08-12-stanford-ml-wk8/image-20190807174016434.png)

#### 算法的严谨定义

K-平均算法的输入有两部分组成，数字$$K$$，代表要分成多少类，以及训练集$$\{x^{(1)},x^{(2)},\cdots,x^{(m)}\}$$。每一个训练数据$$x^{(i)}\in\mathbb{R}^n$$（忽略$$x_0=1$$）。

运行算法前，我们先随机初始化$$K$$个聚类中心$$\mu_1,\mu_2,\cdots,\mu_k\in\mathbb{R}^n$$

$$
\begin{eqnarray*}
\text{re}\text{peat\{}&& \\
&\text{for }&i=1\text{ to }m \\
&\quad&c^{(i)}:=\text{index (from }1\text{ to }K\text{) of cluster centroid closest to }x^{(i)} \\
&\text{for }&k=1\text{ to }K \\
&\quad&\mu_k:=\text{average (mean) of points assigned to cluster }k \\
\text{\}}&&
\end{eqnarray*}
$$

其中，对于第一个for循环，我们称这一步为cluster assignment step。$$c^{(i)}=\min_k\vert\vert x^{(i)}-\mu_k\vert\vert^2$$，即$$c^{(i)}$$储存的是离$$x^{(i)}$$最近的中心$$\mu_k$$的编号$$k$$。例如，如果$$x^{(1)}$$离中心$$\mu_2$$最近，那么$$c^{(1)}=2$$。

对于第二个for循环，我们称这一步为move centroid。这一步$$\mu_k$$的计算不太好用公式表示，这里举个例子说明。假设$$c^{(1)}$$、$$c^{(5)}$$、$$c^{(6)}$$、$$c^{(10)}$$的值都是$$2$$，即$$x^{(1)}$$、$$x^{(5)}$$、$$x^{(6)}$$、$$x^{(10)}$$都最靠近中心$$\mu_2$$。那么$$\mu_2$$的新值就是$$\frac14(x^{(1)}+x^{(5)}+x^{(6)}+x^{(10)})\in\mathbb{R}^n$$。

有时候会出现某个中心没有对应的数据点的情况。在这种情况下，我们可以删掉这一个中心，或者再次随机初始化这个中心。

#### K-平均算法与不可分类

K-平均算法也可以用于如下图所示的，没有明显分界场景

![image-20190807183611364](/assets/2019-08-12-stanford-ml-wk8/image-20190807183611364.png)

假设我们想知道我们应该怎样划分S码、M码和L码，我们同样可以使用K-平均算法，并获得对应的结果。

![image-20190807183725436](/assets/2019-08-12-stanford-ml-wk8/image-20190807183725436.png)

## 深入了解K-平均算法

### 优化目标（代价函数）

$$
J(c^{(1)},\cdots,c^{(m)},\mu_1,\cdots,\mu_K)=\frac1m\sum_{i=1}^m\vert\vert x^{(i)}-\mu_{c^{(i)}}\vert\vert^2
$$

其中，$$\mu_{c^{(i)}}$$的含义如下，假设$$x^{(i)}$$被分配到了第$$k$$个类别中，那么对应的$$c^{(i)}=k$$，$$\mu_{c^{(i)}}=\mu_k$$，即$$\mu_{c^{(i)}}$$是指$$x^{(i)}$$所属类别的中心。

确立了代价函数之后，我们再来重新回顾算法的两部分。首先是cluster assignment step。实际上这一步是在通过调整$$c^{(1)},\cdots,c^{(m)}$$来最小化$$J(\cdots)$$；类似地，在move centroid这一步里，实际上是在通过调整$$\mu_1,\cdots,\mu_k$$来最小化$$J(\cdots)$$。此外，$$J(\cdots)$$有时也称为distortion function。

### 随机初始化

首先我们要确保$$K<m$$（$$K$$是类别的数量，$$m$$是样本的数量），然后我们随机选择$$K$$个训练样本作为我们的中心。然而，如果我们的运气不佳，有可能我们的$$J(c^{(1)},\dots,c^{(m)},\mu_1,\dots,\mu_K)$$只能收敛到局部最小值。

![image-20190811155200526](/assets/2019-08-12-stanford-ml-wk8/image-20190811155200526.png)

其中一种解决方法是多次运行（50-1000次）K-平均算法，每次都用不同的随机初始值，最后选择$$J$$的值最小的作为参数。如果你的类别的数量比较小（小于等于10），这个方法会比较有用；反之，这个方法的效果不是很显著。

### 选择类别的数量

虽然在选择$$K$$的值上，我们没有特别好的方法，但是下面还是介绍一个叫做手肘法（elbow method）的方法来确定$$K$$的数量的大小。

![image-20190811160553786](/assets/2019-08-12-stanford-ml-wk8/image-20190811160553786.png)

$$K=3$$的点就被称为手肘（可以想像$$K=8$$的地方是我们的手，这个图像的曲线很像我们的整个手臂）。但是有时候手肘法的这个转折点不会特别明显，这时候就不太好从这个方法中得出结论了。

![image-20190811160814299](/assets/2019-08-12-stanford-ml-wk8/image-20190811160814299.png)

除了手肘法，还有另一种确定$$K$$值的方法。有时候我们使用K-平均算法是为了能够解决其它的问题（例如前面提到的衣服尺码问题）。

![image-20190811161205123](/assets/2019-08-12-stanford-ml-wk8/image-20190811161205123.png)

对于这种问题，我们可以根据市场调研等方式得到相应的类别的数量。

## 降维（Dimensionality Reduction）

### 介绍

降维可以帮助我们对数据进行压缩，节省占用的空间。它也可以帮助我们加速学习算法的运行。此外，降维也可以帮助我们清理冗余的特征，如下图所示

![image-20190811162906220](/assets/2019-08-12-stanford-ml-wk8/image-20190811162906220.png)

在上图中，我们就将原来的$$x_1$$和$$x_2$$这两个相关的特征映射成了一个特征$$z_1$$。除了将数据从二维降至一维，我们同样也能把数据从三维降到二维，如下图所示

![image-20190811163434291](/assets/2019-08-12-stanford-ml-wk8/image-20190811163434291.png)

在左图中，我们发现，虽然数据是在三维的空间里，但是它们的分布都很靠近某一个平面。于是，在中间的图里，我们将原数据映射到了空间中的某个平面。右图则是我们在这个平面中重新建立新的坐标系$$z_1$$和$$z_2$$，得到新的二维特征$$z$$。

降维除了可以进行数据压缩，它对于数据的可视化也很有帮助。我们的数据很有可能是高维的，比如下面的国家之间的经济数据，有很多特征

![image-20190811164342325](/assets/2019-08-12-stanford-ml-wk8/image-20190811164342325.png)

但是我们想要将国家之间的状况表示出来，所以我们需要将原来的50维的数据降维到二维或者三维的空间里。

![image-20190811164242446](/assets/2019-08-12-stanford-ml-wk8/image-20190811164242446.png)

在上面的例子中，降维得到的$$z_1$$代表了国家的总经济体大小；$$z_2$$代表了人均的财富情况。

## 主成分分析（Principal Component Analysis）

### 介绍

顾名思义，主成分分析，就是通过提取出特征中的主要成分，以实现降维。

![image-20190811201418062](/assets/2019-08-12-stanford-ml-wk8/image-20190811201418062.png)

在上图中，红色和紫色的低维空间（在这里是一条直线）都可以从高维空间（$$x_1$$和$$x_2$$组成的平面）投影得到。但是PCA的最终结果会是红色的线，这是原数据点离红线的距离较小，对应的数据点在红线上依然比较分散，特征比较明显。反之，原数据点离紫色线的距离较大，原本很散的几个数据点都被投影到了一起，损失了比较重要的信息。

![image-20190811182953725](/assets/2019-08-12-stanford-ml-wk8/image-20190811182953725.png)

类似地，在从$$n$$维到$$k$$维的降维过程中，也是找出一些向量$$u^{(1)},u^{(2)},\dots,u^{(k)}$$，并将原空间投影到由这些向量张成的空间中。

此外，吴教授也强调了，PCA与线性回归的不同。线性回归是找出一个预测函数，根据给定的$$x$$，给出相应的$$y$$。而PCA运用的情形是没有$$y$$存在的，它只是对特征$$x$$进行更改。

![image-20190811183447692](/assets/2019-08-12-stanford-ml-wk8/image-20190811183447692.png)

上图中左侧是线性回归，右侧是PCA。注意坐标轴以及误差的计算方式。

### 算法实现

在应用PCA之前，我们首先需要进行特征缩放以及标准化处理（相当于使用对应的z-score来进行PCA）。

![image-20190811184853388](/assets/2019-08-12-stanford-ml-wk8/image-20190811184853388.png)

在应用PCA的时候，我们需要有计算新空间的基底（$$u^{(1)},u^{(2)},\cdots,u^{(k)}$$）以及对应的新特征（$$z^{(1)},z^{(2)},\cdots,z^{(m)}$$）的方式。

![image-20190811185201711](/assets/2019-08-12-stanford-ml-wk8/image-20190811185201711.png)

具体来说，首先我们需要计算协方差矩阵（covariance matrix）

$$
\Sigma=\frac1m\sum_{i=1}^n(x^{(i)})(x^{(i)})^T\in\mathbb{R}^{n\times n}
$$

注：协方差是描述两个随机变量变化程度的量；方差是协方差的特殊形式（两个随机变量为同一个）。

注2：注意区分大写的希腊字母Sigma（$$\Sigma$$）和求和符号（$$\sum$$）。

注3：向量化表示如下

$$
\Sigma=\frac1mX^TX
$$

（$$X$$是$$m\times n$$的矩阵，其中的第$$i$$行为$$(x^{(i)})^T$$）

然后，我们需要计算协方差矩阵的特征向量（eigenvector，这个是线性代数中的特征向量）

```matlab
[U, S, V] = svd(Sigma);
% 注：这里的svd指的是奇异值分解（Singular Value Decomposition）
```

得到的矩阵`U`同样是$$n\times n$$矩阵。如果我们需要将维度降到$$k$$维，我们只需要取`U`中前$$K$$列的向量，得到一个$$n\times k$$的矩阵，我们将这个矩阵表示为$$U_\text{reduce}$$。最终我们的新特征向量$$z$$可以这样得到

$$
z^{(i)}=U_\text{reduce}^Tx^{(i)}
$$

```matlab
U_reduce = U(:, 1:k);
z = U_reduce' * x;
```

#### PCA的数学原理简介

吴教授在这里并没有对PCA的数学原理作过多解释。我自己上网搜索了一番，大概总结如下。

##### 参考资料

- [A Step by Step Explanation of Principal Component Analysis](https://towardsdatascience.com/a-step-by-step-explanation-of-principal-component-analysis-b836fb9c97e2)
- [A Tutorial on Principal Component Analysis](https://arxiv.org/pdf/1404.1100.pdf)
- [主成分分析PCA算法：为什么去均值以后的高维矩阵乘以其协方差矩阵的特征向量矩阵就是“投影”？ - 董理的回答 - 知乎](https://www.zhihu.com/question/30094611/answer/46851173)
- [如何理解不同特征值对应的特征向量线性无关？ - 忆臻的文章 - 知乎](https://zhuanlan.zhihu.com/p/30454490)
- [特征值和特征向量 - 维基百科](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F)
- [为什么随机变量$$X$$和$$Y$$不相关却不一定独立 - 知乎](https://www.zhihu.com/question/26583332)
- [如何去直观地理解不相关不一定是独立，而独立必然不相关 - 知乎](https://www.zhihu.com/question/29641138)
- [特征值和特征向量 - 维基百科](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E5%80%BC%E5%92%8C%E7%89%B9%E5%BE%81%E5%90%91%E9%87%8F)
- [特征分解 - 维基百科](https://zh.wikipedia.org/wiki/%E7%89%B9%E5%BE%81%E5%88%86%E8%A7%A3)
- [奇异值分解 - 维基百科](https://zh.wikipedia.org/wiki/%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3)
- [矩阵的奇异值与特征值有什么相似之处与区别之处 - 知乎](https://www.zhihu.com/question/19666954)
- [Why is the Eigenvector of A Covariance Matrix Equal to A Principal Component](https://math.stackexchange.com/questions/23596/why-is-the-eigenvector-of-a-covariance-matrix-equal-to-a-principal-component)

首先我们要理解什么是“主成分”。其实主成分分析并不一定要降维，实际上它只是将原特征按照一定规则线性组合后，生成的主成分。但是这个主成分有个特点，它的第一个维度上的特征，可能涵盖了绝大部分的信息，越高的维度包含的信息则越少，甚至到最后少到可以忽略不计。

![image-20190811201407545](/assets/2019-08-12-stanford-ml-wk8/image-20190811201407545.png)

用上面这个熟悉的图作为例子。实际上，经过主成分分析之后，我们得到的新的主成分的数量是跟原特征数量相同的。比如说上面原特征$$x\in\mathbb{R}^2$$，新的主成分依然是$$u\in\mathbb{R}^2$$。它的第一维$$u_1$$实际上是上图中的红色线，而它的第二维$$u_2$$是上图中的紫色线（$$u_1$$和$$u_2$$一定是正交的）。在前面我们知道主成分的第一维涵盖的信息是最多的，越往后涵盖的信息越少。我们在这里可以发现，数据点最主要的分散方向，正好是我们第一维$$u_1$$的方向，也就是红色的直线。但是，我们会发现，数据点除了沿着$$u_1$$方向分散之外，其实沿着$$u_2$$方向也有点分散，但是程度远小于$$u_1$$。而我们在使用原坐标系（$$x_1$$和$$x_2$$）时，很难准确地表示这种分散。所以，主成分分析可以理解为，通过线性变换，将数据的分布情况在每个维度（成分）上更好地表示出来。

如果我们将图片旋转一下，用$$u_1$$和$$u_2$$作为横轴和纵轴，会更明显。

![rotated](/assets/2019-08-12-stanford-ml-wk8/rotated.png)

因为$$u_2$$所涵盖的信息量比较小，所以我们可以放心地抛弃它，而不损失关键的信息。这样，我们就成功地将二维的数据降到一维了。

那么我们怎么知道我们的主成分$$u_1$$和$$u_2$$就是最好的呢？直观上来解释，既然我们的目标是让数据尽可能分散，我们可以使用统计学中的协方差（Covariance）的概念来表示这些数据到底有多分散。我们用$$\text{cov}(X, Y)$$表示随机变量$$X$$、$$Y$$之间的协方差。它们的协方差越大，说明它们同向变化的程度越大；越小（负数），说明反向变化的程度越大；协方差为零代表这两个变量不相关（并不是说这两个变量在统计上是独立的，因为貌似统计里面的这个相关是特指线性相关，不是线性相关可以是其他形式的相关，而独立是指一点关系都没有）。计算公式如下

$$
\begin{eqnarray*}
\mathrm{cov}(X,Y)&=&\mathrm{E}[(X-\mathrm{E}[X])(Y-\mathrm{E}[Y])] \\
&=&\mathrm{E}[XY]-\mathrm{E}[X]\mathrm{E}[Y] \\
&=&\mathrm{cov}(Y,X)
\end{eqnarray*}
$$

特别地，如果两个随机变量为同一个，则它们之间的协方差为这个随机变量的方差。

$$
\sigma^2_X\equiv\mathrm{var}(X)=\mathrm{cov}(X,X)
$$

我们可以按照协方差的概念，计算出每一个特征之间的协方差，然后把它们塞进一个叫协方差矩阵的矩阵里头（其实下面的协方差矩阵$$\Sigma$$就是随机向量$$x$$的方差，推广过程可以参考[Variance - Wikipedia](https://en.wikipedia.org/wiki/Variance#Generalizations)）。假设我们我们的原数据有三个特征，那么对应的协方差矩阵如下

$$
\Sigma=\begin{bmatrix}
\mathrm{cov}(x_1,x_1)& \mathrm{cov}(x_1,x_2)& \mathrm{cov}(x_1,x_3) \\
\mathrm{cov}(x_2,x_1)& \mathrm{cov}(x_2,x_2)& \mathrm{cov}(x_2,x_3) \\
\mathrm{cov}(x_3,x_1)& \mathrm{cov}(x_3,x_2)& \mathrm{cov}(x_3,x_3)
\end{bmatrix}
$$

因为协方差是可交换的，因此$$\Sigma=\Sigma^T$$，即$$\Sigma$$是一个实对称矩阵。实对称矩阵的特征向量是正交的。虽然吴教授在介绍代码的时候使用的是奇异值分解，与特征分解有不同之处，但是因为我们这里的$$\Sigma$$是实对称矩阵，所以他们用起来产生的结果是一样的。因为我看不懂这个奇异值分解，所以我就拿特征分解来帮助自己理解了。

我们的协方差矩阵$$\Sigma$$可以按照如下形式分解

$$
\Sigma=UDU^{-1}
$$

其中，$$U$$和$$D$$跟$$\Sigma$$一样，都是$$n\times n$$的矩阵。$$U$$中的每一列都是$$\Sigma$$的特征向量，$$D$$为对角线矩阵（即非对角线上的元素都为零）。$$D$$中从左上到右下，第$$i$$个对角线元素是$$U$$中从左到右，第$$i$$列特征向量的特征值。

好了，我们现在得到了协方差矩阵的特征值和特征向量。问题是这两个东西怎么帮助我们找出主成分呢？原理简单来讲的话，就是有最大特征值的特征向量，正好是使得数据方差最大的方向。

举个例子，假设我们很暴力地把数据降维到只有一维。一个一维的空间只需要一个向量作为基底，这里设这个基底为$$u_1\in\mathbb{R}^n$$。那么对于原$$n$$维空间中的向量（数据点）$$x^{(i)}\in\mathbb{R}^n$$来说，它在新的一维空间中的坐标是$$u_1^Tx^{(i)}\in\mathbb{R}$$。

当然了，我们的$$u_1$$不能乱选，我们希望的是在降维之后，尽可能地保留更多信息，也就是说要尽可能最大化降维后的数据的方差。易得降维后数据方差为$$u_1^T\Sigma u_1$$，证明如下

$$
\begin{eqnarray*}
\mathrm{var}(u_1^Tx)&=&\mathrm{E}[u_1^Txx^Tu_1]-\mathrm{E}[u_1^Tx]\mathrm{E}[x^Tu_1] \\
&=&u_1^T\mathrm{E}[xx^T]u_1-u_1^T\mathrm{E}[x]\mathrm{E}[x^T]u_1 \\
&=&u_1^T(\mathrm{E}[xx^T]-\mathrm{E}[x]\mathrm{E}[x^T])u_1 \\
&=&u_1^T(\mathrm{var}(xx^T))u_1 \\
&=&u_1^T\Sigma u_1
\end{eqnarray*}
$$

这里能把$$u_1$$从期望的运算中提出来是因为数学期望是线性的，具体可以参考[Expected Value - Wikipedia](https://en.wikipedia.org/wiki/Expected_value#Linearity)。在我们知道怎么表示方差后，我们就需要找出这个$$u_1$$了。严格的求解需要用到拉格朗日乘数（我也不懂这是啥），但是如果我们需要直观地理解如何选择我们的$$u_1$$，根据特征向量和特征值的性质，我们可以将特征向量看成是数据伸展的方向，对应的特征值则是这个特征向量伸展的程度。因此，我们选择特征值最大的特征向量作为我们的$$u_1$$，因为它能使数据伸展得最开。

上面是将数据降到一维的情况。如何将这个方法推广到降至$$k$$维呢？我们可以在确定完第一个主成分之后，只需要手动消灭在这个主成分上的方差即可。我本人自己选择用$$(x')^{(i)}$$代表$$x^{(i)}$$在移除第一个主成分后的数据点。

$$
(x')^{(i)}=x^{(i)}-\frac{x^{(i)}\cdot u_1}{u_1\cdot u_1}u_1
$$

![image-20190811232743434](/assets/2019-08-12-stanford-ml-wk8/image-20190811232743434.png)

因为我们的数据是标准化（normalized）过的，所以当我们对所有数据点都进行上述操作后，我们就能得到相应的绿色向量。将这些绿色向量的起点都放置在原点的话，我们会发现，现在数据点在$$u_1$$方向上的方差已经为0（因为$$x$$与它在$$u_1$$上的投影的差值，$$x'$$，是跟$$u_1$$正交的），并且在一个新的方向上（在上面的例子中就是沿着紫色线）展示出了比较明显的（肯定没有原来$$u_1$$方向上那么明显）的伸展。这个方向就是我们的第二个主成分，$$u_2$$。用同样的方法我们可以证明$$u_2$$就是第二大的特征值对应的特征向量。

因此，我们只需要将$$\Sigma=UDU^{-1}$$中$$D$$的对角线元素从左上到右下按从大到小的方式排列，$$U$$中特征向量的顺序也要相应调整，最后得到的$$U$$中，从左到右就是我们的主成分了，其中最左的那一列是信息量最大的组成分，最右的是信息量最小的主成分。最后，我们可以查看这些特征向量对应的特征值，如果特征值特别小，那么说明对应的特征向量（主成分）基本没有太多信息，可以放心舍去。

~~看来吴教授不讲数学原理是有道理的，像我们这种初学者想要完全弄明白还是有点困难，我现在其实还是有一些地方没弄懂，只能说是比完全不懂要好一点的情况。~~

### 数据重构（Reconstruction from Compressed Representation）

现在我们知道如何对数据进行降维（压缩），我们把压缩后的数据给近似恢复到原来的空间中的方法其实也很类似。

$$
x^{(i)}_\text{approx}=U_\text{reduce}z^{(i)}
$$

### 主成分数量（$$k$$）的选择

PCA算法希望能够最小化压缩引入的误差（average squared projection error），即最小化下面的式子

$$
\frac1m\sum_{i=1}^m\vert\vert x^{(i)}-x^{(i)}_\text{approx}\vert\vert^2
$$

此外我们还可以计算数据的总方差

$$
\frac1m\sum_{i=1}^m\vert\vert x^{(i)}\vert\vert^2
$$

我们希望压缩引入的误差与总方差的比值小于某个数，比如说0.01（1%）

$$
\frac{\frac1m\sum_{i=1}^m\vert\vert x^{(i)}-x^{(i)}_\text{approx}\vert\vert^2}{\frac1m\sum_{i=1}^m\vert\vert x^{(i)}\vert\vert^2}\le0.01
$$

对于满足条件的PCA，我们说“99%的方差被保留了”。其它值也是可以的，比如说0.05，0.1等。

在知道如何评判$$k$$的好坏后，我们可以开始实现PCA算法。暴力方法基本上就是从$$k=1$$开始尝试，检查比值是否满足要求，不满足的话给$$k$$增加一，再试，直到遇到能满足要求的$$k$$。当然，如果你仔细读了我自己的补充部分，你会知道，在特征分解（奇异值分解）时，我们会得到一个对角矩阵，里面是每个特征向量对应的特征值（在前面的部分我用$$D$$表示这个矩阵，这里吴教授用$$S$$表示）。所以，我们只需要计算满足要求的$$k$$值即可。式子如下

$$
1-\frac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}}\le0.01
$$

这样无需重复运行PCA也能得到满足“保留了99%方差”条件的$$k$$值。

### 关于PCA应用的建议

#### 利用PCA加速监督学习

假设我们的数据样本的维度特别大（例如$$x^{(i)}\in\mathbb{R}^{10000}$$），如果不使用PCA降维的话，传统的学习算法会跑得比较慢。我们可以首先将$$x^{(i)}$$从$$(x^{(i)},y^{(i)})$$中提取出来，利用PCA得到对应的$$z^{(i)}$$，最后用$$(z^{(i)},y^{(i)})$$作为训练数据进行学习。需要注意的是，我们只能从训练集中得到PCA，我们不能使用交叉验证集或者测试集来获得PCA。但是一旦我们得到了PCA，我们可以将PCA应用到交叉训练集和测试集中来检验。

#### 利用PCA进行数据可视化

一般选择$$k=2$$或者$$k=3$$。对于如何解读通过PCA绘制的图表，可以参考[Principal Component Analysis](http://setosa.io/ev/principal-component-analysis/)这篇文章，文章的结尾有一个通过将17维的数据降至二维后发现outlier的例子。

#### 不建议使用PCA来预防过拟合

PCA可以拿来预防过拟合的原因是因为特征数量减少了，因此就减少了过拟合的可能性。吴教授的建议是如果想要处理过拟合，使用正规化（regularization）。

#### 不建议使用PCA来设计机器学习系统

理由跟之前（第六周）提到的类似，吴教授建议在开始设计一个系统的时候，从简单的功能开始，直到用简单的方法无法解决问题，才考虑使用PCA。

---
layout: post
title:  "斯坦福机器学习课程笔记（Week 6）"
date:   2019-08-01 16:50:00 -0700
tags:   study-cs machien-learning
---

## 本系列的其它文章

- [斯坦福机器学习课程笔记（Week 1）]({% post_url 2019-06-25-stanford-ml-wk1 %})
- [斯坦福机器学习课程笔记（Week 2）]({% post_url 2019-07-05-stanford-ml-wk2 %})
- [斯坦福机器学习课程笔记（Week 3）]({% post_url 2019-07-13-stanford-ml-wk3 %})
- [斯坦福机器学习课程笔记（Week 4）]({% post_url 2019-07-15-stanford-ml-wk4 %})
- [斯坦福机器学习课程笔记（Week 5）]({% post_url 2019-07-25-stanford-ml-wk5 %})
- **斯坦福机器学习课程笔记（Week 6）**
- [斯坦福机器学习课程笔记（Week 7）]({% post_url 2019-08-05-stanford-ml-wk7 %})
- *斯坦福机器学习课程笔记（Week 8）（敬请期待）*
- *斯坦福机器学习课程笔记（Week 9）（敬请期待）*
- *斯坦福机器学习课程笔记（Week 10）（敬请期待）*
- *斯坦福机器学习课程笔记（Week 11）（敬请期待）*

## 机器学习算法诊断

**诊断（Diagnostic）**是一个能让我们感知到某个学习算法是否正常工作的测试，我们可以根据他来改进我们的算法。写一个诊断同样也需要时间，但是它能为我们节省更多的时间。

### 预测函数的评估

一种常用的方法是将我们的数据集随机分成两部分，一部分（70%）用作训练集，一部分（30%）用作测试集。我们用$$m_\text{test}$$来表示测试集数据的数量，$$(x^{(i)}_\text{test}, y^{(i)}_\text{test})$$表示第$$i$$个测试数据。

划分好数据集后，跟往常一样，使用训练集进行训练，最小化代价函数$$J(\theta)$$。训练完成后，将测试集中的数据运用到得到的预测函数上，得到测试集误差$$J_\text{test}(\theta)$$，具体公式如下

$$
J_\text{test}(\theta)=\frac1{2m_\text{test}}\sum_{i=1}^{m_\text{test}}(h_\theta(x^{(i)}_\text{test})-y_\text{test}^{(i)})^2
$$

如果是logistic回归问题（分类问题），测试集误差由下面的公式定义

$$
J_\text{test}(\theta)=-\frac1{m_\text{test}}\sum_{i=1}^{m_\text{test}}[y_\text{test}^{(i)}\log h_\theta(x_\text{test}^{(i)})+(1-y_\text{test}^{(i)})\log(1-h_\theta(x_\text{test}^{(i)}))]
$$

或者也可以由下面的公式定义

$$
\begin{eqnarray*}
\text{err}(h_\theta(x),y)&=&\begin{cases}
1& \text{if }h_\theta(x)\ge0.5\text{ while }y=0 \\
& \quad\text{or if }h_\theta(x)<0.5\text{ while }y=1 \\
0& \text{otherwise}
\end{cases} \\
J_\text{test}(\theta)&=&\frac1{m_\text{test}}\sum_{i=1}^{m_\text{test}}\text{err}(h_\theta(x_\text{test}^{(i)}),y^{(i)}_\text{test})
\end{eqnarray*}
$$

我们称这里的$$\text{err}(h_\theta(x), y)$$为错分类误差（Misclassification Error），这么做得到的$$J_\text{test}(\theta)$$是测试中误分类的比率。

### 模型选择

当我们使用多项式回归时，我们需要确定我们多项式的次数。假设我们使用的最高次数是10次，则代表我们将从下列预测函数中进行选择

$$
\begin{eqnarray*}
h_\theta(x)&=&\theta_0+\theta_1x \\
h_\theta(x)&=&\theta_0+\theta_1x+\theta_2x^2 \\
h_\theta(x)&=&\theta_0+\theta_1x+\cdots+\theta_3x^3 \\
&\vdots& \\
h_\theta(x)&=&\theta_0+\theta_1x+\cdots+\theta_{10}x^{10}
\end{eqnarray*}
$$

我们用$$\Theta^{(d)}$$表示第$$d$$次的多项式的所有参数，用$$J_\text{test}(\Theta^{(d)})$$表示对应的测试集误差。假设次数为5次的时候，测试集误差最小，但是我们并不能因此说五次多项式是最合适的，因为测试集误差最小只说明了$$\Theta^{(5)}$$比较适合测试集。

为了解决这个问题，我们可以将数据集分成三部分，训练集（60%），交叉验证集（Cross Validation Set，20%），以及测试集（20%）。我们用$$(x_\text{cv}^{(i)}, y_\text{cv}^{(i)})$$表示交叉验证集中的第$$i$$个数据，用

$$
J_\text{cv}(\theta)=\frac1{2m_\text{cv}}\sum_{i=1}^{m_\text{cv}}(h_\theta(x^{(i)}_\text{cv})-y_\text{cv}^{(i)})^2
$$

来表示交叉验证集的误差。将数据分类后，我们将使用交叉验证集来进行模型选择（而不是测试集）。具体流程如下，假设同样最高次数为10次，同样我们有十个不同的预测函数可以选择。同样用相同的符号表示参数和误差，只是这次我们挑选使$$J_\text{cv}(\Theta^{(d)})$$最小的$$d$$，最后我们再根据$$J_\text{test}(\Theta^{(d)})$$来预测我们模型的误差。

## 误差（Bias）和方差（Variance）

![image-20190729235445533](/assets/2019-08-01-stanford-ml-wk6/image-20190729235445533.png)

误差高说明了模型无法反映数据的特征，方差高说明了模型过拟合，过于敏感，无法推广使用。对于多项式回归，如果我们将误差和次数的值的关系绘制出来，会得到如下图像

![image-20190730000428412](/assets/2019-08-01-stanford-ml-wk6/image-20190730000428412.png)

当$$d$$较小，但是$$J_\text{cv}$$和$$J_\text{train}$$都很大时，很有可能我们的模型出现了高误差（欠拟合）问题；当$$d$$较大，且$$J_\text{cv}$$比$$J_\text{train}$$大很多时，很有可能我们的模型出现了高方差（过拟合）问题。

### 正规化（Regularization）与偏差和方差

当我们的$$\lambda$$过大时，对高次项的惩罚比较大，有可能导致高偏差；相反，当$$\lambda$$过小时，容易产生高方差。

![image-20190730001159412](/assets/2019-08-01-stanford-ml-wk6/image-20190730001159412.png)

我们可以通过尝试多个不同$$\lambda$$值的方法来挑选合适的值。对于每一个候选$$\lambda$$值，我们都计算出它对应的使得$$J_\text{train}$$最小的$$\Theta$$。再将得到的$$\Theta$$带入到交叉验证集中，计算出每个$$\Theta$$对应的交叉验证集误差$$J_\text{cv}$$，最后选择$$J_\text{cv}$$最小的$$\Theta$$作为最终的参数，并计算测试集误差。

![image-20190730001644786](/assets/2019-08-01-stanford-ml-wk6/image-20190730001644786.png)

如果我们观察$$\lambda$$与$$J_\text{train}$$、$$J_\text{cv}$$的关系，我们会发现如下规律

![image-20190730002037309](/assets/2019-08-01-stanford-ml-wk6/image-20190730002037309.png)

当$$\lambda$$较小时，$$J_\text{train}$$较小，$$J_\text{cv}$$较大（过拟合高方差）；当$$\lambda$$较大时，$$J_\text{train}$$和$$J_\text{cv}$$都很大（欠拟合高偏差）。注意这里的$$J_\text{train}$$和$$J_\text{cv}$$都不需要加$$\lambda$$。

### 学习曲线（Learning Curves）

学习曲线是根据训练集样本数量与误差的关系绘制出来的图像，它可以帮助我们调试机器学习算法。

![image-20190730004334308](/assets/2019-08-01-stanford-ml-wk6/image-20190730004334308.png)

如果$$J_\text{cv}$$很快停止下降并保持在一个较高的水平，并且$$J_\text{train}$$快速上升并跟$$J_\text{cv}$$基本持平时，一般是算法出现了高偏差问题。当我们的算法出现高偏差问题时，增加训练数据是没有用的。

![image-20190730004717958](/assets/2019-08-01-stanford-ml-wk6/image-20190730004717958.png)

如果$$J_\text{train}$$和$$J_\text{cv}$$的关于$$m$$的图像类似上图所示，中间存在一个间隙，算法很有可能出现了高方差问题。此时，增加训练数据有可能会有用。

注意，这里的$$m$$是指训练集的总大小，不是指迭代次数。

## 下一步该怎么做

| 方法                                                     | 适用场景   |
| :------------------------------------------------------- | ---------- |
| 使用更大的训练集                                         | 出现高方差 |
| 减少特征的数量                                           | 出现高方差 |
| 增加特征的数量                                           | 出现高偏差 |
| 增加多项式特征（例如$$x_1^2$$、$$x_2^2$$、$$x_1x_2$$等） | 出现高偏差 |
| 增加$$\lambda$$                                          | 出现高方差 |
| 减少$$\lambda$$                                          | 出现高偏差 |

## 神经网络与过拟合

对于较简单的神经网络（参数较少），容易出现欠拟合问题；对于比较复杂的神经网络（参数很多，比如说有很多层），容易出现过拟合问题（可以通过正规化（$$\lambda$$）来解决）。

在构建神经网络时，我们可以从1个隐藏层开始，然后用类似多项式回归的方法，同时跑多个具有不同层数的神经网络，来确定最后的模型。

## 垃圾邮件分类器——如何设计机器学习系统

### 弄清楚努力的方向

假设我们需要设计一个能够区分垃圾邮件的机器学习算法，我们首先该确定我们的$$x$$，也就是特征，都包括哪些成分。视频中使用了垃圾邮件中的常见词语作为特征。

对特征和结果有一个基本了解之后，接下来我们要弄清楚我们接下来应该做什么。对于垃圾邮件的例子，视频中给出了下列选项

- 收集大量数据
- 细化我们的特征
  - 邮件标题、发件人
  - 邮件正文
  - 解决邮件中为了应付反垃圾邮件系统而故意写错的词

正因为可行的方法有很多，很难说哪一种方法最好。因此，我们需要有一套科学的理论来指导我们找出最优的方法，而不是随便挑一个，弄上大半年，可能效果也不尽人意。

### 误差分析

视频中给出了进行机器学习设计的理论

- 从简单、易实现的算法开始入手，并用交叉验证集进行测试
- 绘制学习曲线，找出当前算法存在的问题（高偏差还是高方差，是否需要更多数据或特征来改进）
- 进行误差分析——手动检查交叉验证集中算法误判的部分，并总结出导致误判的原因

继续用之前邮件的例子，假设交叉训练集中总共有500封邮件，其中有100封分错了。我们可以手动检查这些邮件，找出里面的规律特征。此外，为了更好地评价我们的算法，我们最好能够将算法的结果表示为直观的数字。

## 处理偏斜数据

先来看一个例子。假设我们现在在训练一个logistic回归模型（$$y=1$$代表有癌症，$$y=0$$代表没有癌症），我们发现我们模型的正确率是99%，听上去好像很不错。可是实际上，我们的样本中，只有0.5%的人有癌症。实际上，如果我们令我们的预测函数对所有人都预测成没有癌症，我们的正确率能达到99.5%，甚至比我们训练出来的模型还准确，但是我们都知道这样子是有问题的。我们称这种两边数量差距极大的情况为偏斜数据（Skewed Data）。对于这种偏斜数据，我们需要新的方法来表示我们的误差。

### 准确度（Precision）和召回度（Recall）

|                | $$y=1$$                  | $$y=0$$                  |
| -------------- | :----------------------- | ------------------------ |
| $$h(x)\ge0.5$$ | 真阳性（True Positive）  | 假阳性（False Positive） |
| $$h(x)<0.5$$   | 假阴性（False Negative） | 真阴性（True Negative）  |

$$
\begin{eqnarray*}
\text{Precision}&=&\frac{\text{True positives}}{\text{# predicted as positive}}=\frac{\text{True positives}}{\text{True positives}+\text{False positives}} \\
\text{Recall}&=&\frac{\text{True positives}}{\text{# actual as positive}}=\frac{\text{True positives}}{\text{True positives}+\text{False negatives}}
\end{eqnarray*}
$$

准确度是真阳性结果占所有预测结果为阳性的结果的比例，它体现了我们在作出判断时，到底这个判断有多准。召回度是真阳性占结果占所有实际结果为阳性的结果的比例，它体现了我们到底找出了多少个应该找出来的正确结果（到底有多少漏网之鱼）。

此外，在使用准确度召回度时，一般将$$y=1$$设为罕见的类别。

### 准确度与召回度的权衡

假设为了提高我们模型的准确度，我们提高了$$h(x)$$预测的阈值（例如从0.5提高到0.7）。但是与此同时，模型的召回度便降低了。

假设为了减少漏网之鱼的数量，即提高召回度，我们降低了阈值。但是这么做会让模型的准确度就会降低。

![image-20190801033302534](/assets/2019-08-01-stanford-ml-wk6/image-20190801033302534.png)

准确度召回度的曲线如上图所示，图中多种颜色的曲线代表他们之间的关系不是唯一的。

为了比较不同的准确度召回度组合，我们可以引入F分数（F Score，有时也写成$$F_1$$ Score）来辅助我们判断，它等于$$2\frac{\text{PR}}{\text{P}+\text{R}}$$。当准确度和召回度都为1时，F分数为1；相反，当准确度和召回度都为0时，F分数为0。

## 大数据集

虽然说不要盲目收集数据，但是在特定情况下，收集尽可能多的数据还是非常有帮助的。这里的“特定情况”是指满足下面的条件

- 在确立了模型的特征之后，一个人类专家能不能作出判断自信的判断？如果人类专家也无法作出准确的判断，那么给机器增加数据是没有用的。我们应该考虑如何改进我们的特征
- 如果我们模型非常复杂（比如说有特别多特征，或者神经网络的隐藏单元特别多），那么我们模型的偏差会比较小，那么使用大数据集的话很有可能就能减少模型的方差
